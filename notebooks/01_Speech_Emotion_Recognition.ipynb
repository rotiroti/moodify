{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moodify - Speech Emotion Recognition (SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "\n",
    "# import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import Audio, display\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(\"../data/raw\")\n",
    "fc.filter_pattern = [\"*.wav\"]\n",
    "\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback sample WAV file in case user did not select any via FileChooser\n",
    "FALLBACK_WAV_PATH = \"../data/raw/01-01-05-02-02-01-11.wav\"\n",
    "SAMPLE_WAV_SPEECH_PATH = Path(\n",
    "    fc.selected if fc.selected is not None else FALLBACK_WAV_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    if sample_rate:\n",
    "        print(\"Sample Rate:\", sample_rate)\n",
    "        print(\"Shape:\", tuple(waveform.shape))\n",
    "        print(\"Dtype:\", waveform.dtype)\n",
    "        print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "        print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "        print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "        print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = librosa.load(SAMPLE_WAV_SPEECH_PATH, sr=None)\n",
    "print_stats(waveform, sample_rate, SAMPLE_WAV_SPEECH_PATH)\n",
    "Audio(waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Features using Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_centroid = librosa.feature.spectral_centroid(y=waveform, sr=sample_rate)\n",
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(y=waveform, sr=sample_rate)\n",
    "chromagram = librosa.feature.chroma_stft(y=waveform, sr=sample_rate)\n",
    "spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(waveform)), ref=np.max)\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.suptitle(\"Original Audio Features\")\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "librosa.display.waveshow(waveform, sr=sample_rate)\n",
    "plt.title(\"Waveform\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.semilogy(spectral_centroid.T, label=\"Spectral Centroid\")\n",
    "plt.title(\"Spectral Centroid\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.semilogy(spectral_bandwidth.T, label=\"Spectral Bandwidth\")\n",
    "plt.title(\"Spectral Bandwidth\")\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "librosa.display.specshow(chromagram, sr=sample_rate, x_axis=\"time\", y_axis=\"chroma\")\n",
    "plt.title(\"Chromagram\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "librosa.display.specshow(spectrogram, sr=sample_rate, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.title(\"Spectrogram\")\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "librosa.display.specshow(\n",
    "    librosa.power_to_db(mel_spectrogram, ref=np.max),\n",
    "    sr=sample_rate,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    ")\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Emotion Prediction using Whisper AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_id = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id)\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)\n",
    "id2label = model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the waveform to 16kHz\n",
    "target_sr = feature_extractor.sampling_rate\n",
    "resampled_waveform = librosa.resample(\n",
    "    y=waveform, orig_sr=sample_rate, target_sr=target_sr\n",
    ")\n",
    "\n",
    "spectral_centroid = librosa.feature.spectral_centroid(\n",
    "    y=resampled_waveform, sr=target_sr\n",
    ")\n",
    "spectral_bandwidth = librosa.feature.spectral_bandwidth(\n",
    "    y=resampled_waveform, sr=target_sr\n",
    ")\n",
    "chromagram = librosa.feature.chroma_stft(y=resampled_waveform, sr=target_sr)\n",
    "spectrogram = librosa.amplitude_to_db(\n",
    "    np.abs(librosa.stft(resampled_waveform)), ref=np.max\n",
    ")\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=resampled_waveform, sr=target_sr)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.suptitle(\"Original Audio Features\")\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "librosa.display.waveshow(resampled_waveform, sr=target_sr)\n",
    "plt.title(\"Waveform\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.semilogy(spectral_centroid.T, label=\"Spectral Centroid\")\n",
    "plt.title(\"Spectral Centroid\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.semilogy(spectral_bandwidth.T, label=\"Spectral Bandwidth\")\n",
    "plt.title(\"Spectral Bandwidth\")\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "librosa.display.specshow(chromagram, sr=target_sr, x_axis=\"time\", y_axis=\"chroma\")\n",
    "plt.title(\"Chromagram\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "librosa.display.specshow(spectrogram, sr=target_sr, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.title(\"Spectrogram\")\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "librosa.display.specshow(\n",
    "    librosa.power_to_db(mel_spectrogram, ref=np.max),\n",
    "    sr=target_sr,\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    ")\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(\n",
    "    resampled_waveform,\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()\n",
    "emotion_labels = model.config.id2label\n",
    "emotion_scores = {emotion_labels[i]: probs[i].item() for i in range(len(probs))}\n",
    "emotion_scores = dict(\n",
    "    sorted(emotion_scores.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "emotions = list(emotion_scores.keys())\n",
    "scores = list(emotion_scores.values())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x=emotions, y=scores, palette=\"viridis\", hue=emotions)\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Confidence Score\")\n",
    "plt.title(\"Emotion Predictions\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEmotion Predictions:\\n\")\n",
    "for emotion, score in emotion_scores.items():\n",
    "    print(f\"- {emotion}: {score:.4f}\")\n",
    "\n",
    "top_emotion_index = torch.argmax(probs).item()\n",
    "top_emotion = emotion_labels[top_emotion_index]\n",
    "\n",
    "print(\n",
    "    f\"\\nPredominant Emotion: {top_emotion} (Confidence: {emotion_scores[top_emotion]:.4f})\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
